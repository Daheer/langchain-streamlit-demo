# langchain-streamlit-demo

This demo showcases the enhancement that connecting LLMs to services Google Search can allow.

This demo uses audio as input. The audio is then transcribed using OpenAI's Whisper model. This transcribed text is then used to prompt Open AI's GPT-3, but not just the regular GPT-3, one that has access to Google Search. It's able to determine whether answering the prompt requires a Google search for fact checks.

[Read article on LinkedIn](https://www.linkedin.com/pulse/introducing-langchain-connect-amplify-power-llms-dahiru-ibrahim/?trackingId=W2AN8vwlQR%2BPOpT90iyGSw%3D%3D)



<br>

# Built Using

- [Python](https://python.org)
- [Langchain](https://langchain.readthedocs.io/)
- [OpenAI Whisper](https://openai.com)
- [Streamlit](https://streamlit.io/)


# Demo



https://user-images.githubusercontent.com/34832399/225603098-dcfaf68b-6aed-46c8-9227-002e86d35f4d.mp4



# Reference

- [Langchain](https://langchain.readthedocs.io/)
- [Streamlit Audio Recorder](https://github.com/theevann/streamlit-audiorecorder)

# Contact

Dahir Ibrahim (Deedax Inc) - http://instagram.com/deedax_inc <br>
Email - suhayrid6@gmail.com <br>
Twitter - https://twitter.com/DeedaxInc <br>
YouTube - https://www.youtube.com/channel/UCqvDiAJr2gRREn2tVtXFhvQ <br>
Project Link - https://github.com/Daheer/langchain-streamlit-demo

